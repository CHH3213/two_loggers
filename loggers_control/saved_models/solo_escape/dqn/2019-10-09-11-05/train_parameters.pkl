(dp0
S'normalize'
p1
I00
sS'date_time'
p2
S'2019-10-09-11-05'
p3
sS'complete_episodes'
p4
I10000
sS'num_episodes'
p5
I10000
sS'agent_state_dimension'
p6
I7
sS'wall_bonus'
p7
I0
sS'num_steps'
p8
I400
sS'door_bonus'
p9
I0
sS'time_bonus'
p10
I0
sS'source'
p11
S''
p12
sS'agent_action_options'
p13
cnumpy.core.multiarray
_reconstruct
p14
(cnumpy
ndarray
p15
(I0
tp16
S'b'
p17
tp18
Rp19
(I1
(I2
I2
tp20
cnumpy
dtype
p21
(S'i8'
p22
I0
I1
tp23
Rp24
(I3
S'<'
p25
NNNI-1
I-1
I0
tp26
bI00
S'\x01\x00\x00\x00\x00\x00\x00\x00\xff\xff\xff\xff\xff\xff\xff\xff\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00'
p27
tp28
bsS'success_bonus'
p29
I0
sS'success_count'
p30
I5448
sS'agent_layer_sizes'
p31
(lp32
I128
asS'agent_learning_rate'
p33
F0.001
sS'train_dur'
p34
F130781.8755800724
s.